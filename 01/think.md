- java的版本接口有问题。
- 还是python好
- python好慢，还是c++好 
- 数据量大要先跑小数据量进行测试


---

关于随机生成日志遇到的问题以及思考 —— By Eser

- 一开始考虑用MR生成日志，但发生了太多问题（接口使用不熟悉以及业务逻辑等不熟练），后来改成单机版本
- 这里有三种方式，PY, JAVA和C++，考虑到用C的接口可能会快点，权衡之下决定用C++的C接口，开O3优化RELEASE出个可执行文件
- 以下又分成两个方案，一个是在机子上生成TXT日志文件后PUT到HDFS，另外一个是使用HDFS的流接口
	- 流接口方案
		- 在根据官方文档改完代码，并准备用G++编译的时候，出现错误，之后发现官方文档给出命令的某个链接库路径hadoop/lib/native下面一个文件也没有，之后谷歌无果，无奈放弃
	- 本机生成方案
		- 一开始决定生成50亿条数据，大概有150g的文件，因为实验室机子的UBUNTU系统只有45G不到的使用空间，单机生成明显不可行
		- 决定把一天按照时间顺序整块分割成5份，这样每份日志文件大概30G，丢到5个机子分别生成5个LOG，在PUT到HDFS，此处遇到几个问题
			- 一是配置HADOOP的时候，脑子发热在STANDBY也配置了DATANODE，导致这个节点很容易就内存不足
			- 二是在生成LOG文件之后，前5个NODE的内存明显超标，RM显示为UNHEALTHY node，这个时候如果直接再每台机子PUT会让此台机子直接崩溃
				- 此问题后采用逐个LOG先SCP到MASTER，再删掉该NODE的LOG释放大量内存，由MASTER进行PUT到HDFS的操作，此过程为串行+IO，浪费了我大量时间
			- 三就是代码DEBUG原因了，没有充分测试就埋头搞
		- 以上生成的东西有150G，但由于HDFS会有3个拷贝，MR中间过程又会生成大量数据，空间爆炸，无奈放弃
		- 解决：适当缩小生成记录条数，都除以5，也就是生成10亿数据以及160W用户
- 关于生成方案
	- 一开始考虑先随机生成用户，再随机生成两个时间，再随机决定是否LOGIN LOGOUT都有，但需要后期排序，各种开销太大放弃
	- 后来改成假设一天用户行为随机分布，顺序便利一天的每一秒，随机生成一定数量的用户，再随机决定LOGIN LOGOUT

---




---

关于随机生成以及统计的思路 ——By Makdon

- 随机生成日志
    - 使用MapReduce生成
    - 读入的是一个若干行的无意义文件，然后依照行数，Mapper对每一行生成若干个用户的log，以（key：time，value：log）传入reducer
    - reducer读入log之后，进行排序后输出到文件
- 计算平均时间
    - 使用两个MapReduce的Job来进行统计：
    - 第一个job读入log的文件，输出每个用户的在线时间，如（userID， online_time）
    - 第二个job读入上一个job的输出，把所有用户的时间取一个平均值，其中用到combiner进行本地合并加速。
